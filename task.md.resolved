# Project Roadmap: LLMops Multi-Doc Chat

- [ ] **Phase 1: Project Roadmap & Architecture** <!-- id: 0 -->
    - [x] Analyze existing code ([ingest.py](file:///home/gsavaliya/LLMops_Multi_Doc_Chat/ingest.py), [chat.py](file:///home/gsavaliya/LLMops_Multi_Doc_Chat/chat.py)) <!-- id: 1 -->
    - [x] Create detailed implementation plan for FastAPI wrapper <!-- id: 2 -->
    - [x] Define AWS Deployment Strategy (OpenAI for AWS, Ollama for Local) <!-- id: 3 -->
- [ ] **Phase 2: FastAPI & Architecture Refactor** <!-- id: 4 -->
    - [ ] Refactor [chat.py](file:///home/gsavaliya/LLMops_Multi_Doc_Chat/chat.py) to support dual backends (Ollama/OpenAI) <!-- id: 101 -->
    - [ ] Create `api.py` (FastAPI app) <!-- id: 5 -->
    - [ ] Integrate [chat.py](file:///home/gsavaliya/LLMops_Multi_Doc_Chat/chat.py) logic into API endpoints <!-- id: 6 -->
    - [ ] Verify using Swagger UI <!-- id: 7 -->
- [ ] **Phase 3: Custom UI** <!-- id: 8 -->
    - [ ] Design simple UI (e.g., Streamlit or HTML/JS) <!-- id: 9 -->
    - [ ] Connect UI to FastAPI backend <!-- id: 10 -->
- [ ] **Phase 4: Testing & Evaluation (LangSmith & MLflow)** <!-- id: 19 -->
    - [ ] Setup MLflow for experiment tracking <!-- id: 20 -->
    - [ ] Setup LangSmith for tracing and testing <!-- id: 21 -->
    - [ ] Run evaluation dataset on the RAG pipeline <!-- id: 22 -->
- [ ] **Phase 5: Containerization (Docker)** <!-- id: 11 -->
    - [ ] Create `Dockerfile` <!-- id: 12 -->
    - [ ] Create `docker-compose.yml` (App + Vector DB + Ollama/OpenAI Env) <!-- id: 13 -->
    - [ ] Test local Docker build <!-- id: 14 -->
- [ ] **Phase 6: CI/CD & Deployment** <!-- id: 15 -->
    - [ ] Setup GitHub Actions for testing/linting <!-- id: 16 -->
    - [ ] Setup ECR pushing <!-- id: 17 -->
    - [ ] Deploy to AWS Lambda (using OpenAI backend) <!-- id: 18 -->
